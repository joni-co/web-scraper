{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optical-hostel",
   "metadata": {},
   "source": [
    "## Script für Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "textile-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2021-04-23\n",
      "[INFO] Scraped sz (https://www.sueddeutsche.de/)\n",
      "[INFO] Scraped zeit (https://www.zeit.de/index)\n",
      "[INFO] Scraped faz (https://www.faz.net/aktuell/)\n",
      "[INFO] Scraped ts (https://www.tagesspiegel.de/)\n",
      "[INFO] Scraped spiegel (https://www.spiegel.de/)\n",
      "[INFO] Scraped kronen (https://www.krone.at/)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Prepare scraping\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "SOURCES_PATH = os.path.join(\"input\", \"web-sources.csv\")\n",
    "STORAGE_PATH = os.path.join(\"data-lake\")\n",
    "\n",
    "# Read sources\n",
    "web_sources = pd.read_csv(SOURCES_PATH)\n",
    "web_sources.head()\n",
    "\n",
    "# Current date as string\n",
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%Y-%m-%d\")\n",
    "print(\"Date:\", now_str)\n",
    "\n",
    "content_dict = {}\n",
    "text_dict = {}\n",
    "log_list = []\n",
    "failing_list = []\n",
    "\n",
    "def scrape_website(name, url):\n",
    "\n",
    "    # (1) Run request\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    content = response.content\n",
    "    text = response.text\n",
    "\n",
    "    # (2) File name to store the raw HTML\n",
    "    file_name = os.path.join(\n",
    "        STORAGE_PATH,\n",
    "        f\"{now_str}-{name}.html\",\n",
    "    )\n",
    "\n",
    "    # (3) Write raw HTML\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # (4) Fill content_dict and text_dict\n",
    "    content_dict[name] = response.content\n",
    "    text_dict[name] = response.text\n",
    "\n",
    "    # (5) Fill log_list\n",
    "    log_info = dict(\n",
    "        name=name,\n",
    "        date=now_str,\n",
    "        file_name=file_name,\n",
    "        status=response.status_code,\n",
    "        original_url=url,\n",
    "        final_url=response.url,\n",
    "        encoding=response.encoding,\n",
    "    )\n",
    "    log_list.append(log_info)\n",
    "\n",
    "def scrape_wrapper(newspaper):\n",
    "    url = newspaper[\"url\"]\n",
    "    name = newspaper[\"name\"] #id\n",
    "    try:\n",
    "        scrape_website(name, url)\n",
    "        print(f\"[INFO] Scraped {name} ({url})\")\n",
    "    except:\n",
    "        failing_list.append((name, url))\n",
    "        print(f\"[ERROR] Failed to scrape: {name} ({url})\")\n",
    "\n",
    "\n",
    "web_sources.apply(scrape_wrapper, axis=1)\n",
    "\n",
    "log_file_name = os.path.join(\n",
    "    STORAGE_PATH,\n",
    "    f\"{now_str}.csv\",\n",
    ")\n",
    "log_df = pd.DataFrame(log_list)\n",
    "log_df.to_csv(log_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-horizontal",
   "metadata": {},
   "source": [
    "## Script für das DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "static-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing sz\n",
      "[INFO] Processing zeit\n",
      "[INFO] Processing faz\n",
      "[INFO] Processing ts\n",
      "[INFO] Processing spiegel\n",
      "[INFO] Processing kronen\n",
      "Data shape: (10136, 4)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Data Warehouse\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "STORAGE_PATH = os.path.join(\"data-lake\")\n",
    "SQL_PATH = os.path.join(\"dwh.sqlite3\")\n",
    "\n",
    "# Current date as string\n",
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "log_file_name = os.path.join(\n",
    "    STORAGE_PATH,\n",
    "    f\"{now_str}.csv\",\n",
    ")\n",
    "log_file = pd.read_csv(log_file_name)\n",
    "log_file.head()\n",
    "\n",
    "stopwords_url = \"https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt\"\n",
    "stopwords_list = requests.get(stopwords_url, allow_redirects=True).text.split(\"\\n\")[9:]\n",
    "\n",
    "def read_html_file(filename, encoding=\"utf-8\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def process_html(text):\n",
    "    items = text.replace(\"\\n\", \" \").lower().split(\" \")\n",
    "    items = [i for i in items if len(i) > 1 and i not in stopwords_list]\n",
    "    return items\n",
    "\n",
    "def process_newspaper(newspaper):\n",
    "    filename = newspaper[\"file_name\"]\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    text = read_html_file(filename, encoding)\n",
    "    bstext = BeautifulSoup(text, \"html.parser\").text\n",
    "    items = process_html(bstext)\n",
    "    count = pd.Series(items).value_counts().to_frame()\n",
    "    count.columns = [\"count\"]\n",
    "    count[\"word\"] = count.index\n",
    "    count[\"name\"] = newspaper[\"name\"] #paper(1.)\n",
    "    count[\"date\"] = now_str\n",
    "    return count\n",
    "\n",
    "collection = []\n",
    "\n",
    "def process_wrapper(newspaper):\n",
    "    name = newspaper[\"name\"]\n",
    "    try:\n",
    "        count = process_newspaper(newspaper)\n",
    "        print(f\"[INFO] Processing {name}\")\n",
    "        collection.append(count)\n",
    "    except:\n",
    "        print(f\"[ERROR] Failt to process {name}\")\n",
    "\n",
    "log_file.apply(process_wrapper, axis=1)\n",
    "\n",
    "data = pd.concat(collection, axis=0)\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "lockdown = data.loc[data[\"word\"] == \"lockdown\"]\n",
    "lockdown.head()\n",
    "\n",
    "connection = sqlite3.connect(SQL_PATH)\n",
    "data.to_sql(\"wordcount\", connection, index=False, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-wells",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
